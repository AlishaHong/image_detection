{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "046fb49b-a96a-4ce4-8afa-d5e58cc6d4bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/park0/yolov11/customData/org'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, shutil\n",
    "HOME = 'C:/Users/park0/yolov11/customData/'\n",
    "backupPath = os.path.join(HOME, 'backup')\n",
    "\n",
    "# 기존의 폴더가 있으면 삭제 후 다시 생성\n",
    "orgDataPath =  os.path.join(HOME, 'org')\n",
    "if os.path.exists(orgDataPath):\n",
    "    shutil.rmtree(orgDataPath)\n",
    "shutil.copytree(backupPath, orgDataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2050931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "custom_folder = 'customdata'\n",
    "HOME = os.path.join(os.getcwd(), custom_folder)\n",
    "os.makedirs(custom_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8f5b2f2-319e-4eec-97d3-e838a2de9a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SBA\\repository\\image_detection\\snack_dataOrg_640\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "orgDataPath =  os.path.join(os.getcwd(),'snack_dataOrg_640')\n",
    "print(orgDataPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aba07f3-04cd-45ac-adc6-e0363c1b61ca",
   "metadata": {},
   "source": [
    "## 데이터셋 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68af2d16-fc5e-49e3-a6cc-7d757f0ceb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob \n",
    "\n",
    "# 원본 데이터셋이 저장된 폴더\n",
    "imagefiles = glob(os.path.join(orgDataPath, '*.jpg'))\n",
    "labelfiles = glob(os.path.join(orgDataPath, '*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d84c13f-1fea-4809-a28a-12a17ca4b672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 18/18\n"
     ]
    }
   ],
   "source": [
    "# 이미지 파일과 txt파일이 동일하게 한쌍으로 존재하는지 확인\n",
    "correct = 0\n",
    "for i in range(len(imagefiles)):\n",
    "    if imagefiles[i].split('.jpg')==labelfiles[i].split('.txt'):\n",
    "        correct +=1\n",
    "        #print(imagefiles[i], labelfiles[i])\n",
    "print(\"correct: {}/{}\".format(correct,len(imagefiles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86d133e7-59af-4701-ac86-75468e3d26d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70d58901-0e35-4ce2-a4b3-8778b37e94e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOX_COLOR = (255, 0, 0) # Red\n",
    "TEXT_COLOR = (255, 255, 255) # White\n",
    "\n",
    "\n",
    "def visualize_bbox(img, bbox, class_name, width, height, dataType='yolo', color=BOX_COLOR, thickness=2):\n",
    "    \"\"\"Visualizes a single bounding box on the image\"\"\"\n",
    "\n",
    "    if dataType == 'coco':\n",
    "        x_min, y_min, w, h = bbox\n",
    "        \n",
    "        x_min, x_max, y_min, y_max = int(x_min), int(x_min + w), int(y_min), int(y_min + h)\n",
    "\n",
    "    elif dataType =='yolo':\n",
    "        x_center, y_center, w, h = bbox\n",
    "        # 정규화된 값 0~1사이의 값\n",
    "        # 픽셀 좌표로 변환 이미지의 width와 height값을 곱해주면 된다.\n",
    "        x_min = int((x_center - (w/2))*width)\n",
    "        x_max = int((x_center + (w/2))*width)\n",
    "        y_min = int((y_center - (h/2))*height)\n",
    "        y_max = int((y_center + (h/2))*height)\n",
    "    print(x_min, x_max, y_min,y_max)\n",
    "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n",
    "\n",
    "    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)\n",
    "    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)\n",
    "    cv2.putText(\n",
    "        img,\n",
    "        text=class_name,\n",
    "        org=(x_min, y_min - int(0.3 * text_height)),\n",
    "        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        fontScale=0.35,\n",
    "        color=TEXT_COLOR,\n",
    "        lineType=cv2.LINE_AA,\n",
    "    )\n",
    "    return img\n",
    "\n",
    "\n",
    "def visualize(image, bboxes, category_ids, category_id_to_name, img_width, img_height):\n",
    "    img = image.copy()\n",
    "    for bbox, category_id in zip(bboxes, category_ids):\n",
    "        class_name = category_id_to_name[category_id]\n",
    "        img = visualize_bbox(img, bbox, class_name, img_width, img_height, 'yolo')\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb89ad7d-d0b1-42ed-9d6f-14019d01dbb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\SBA\\\\repository\\\\image_detection\\\\snack_dataOrg_640\\\\1_20241008_102733.jpg',\n",
       " 'c:\\\\Users\\\\SBA\\\\repository\\\\image_detection\\\\snack_dataOrg_640\\\\1_20241008_102740.jpg',\n",
       " 'c:\\\\Users\\\\SBA\\\\repository\\\\image_detection\\\\snack_dataOrg_640\\\\1_20241008_102744.jpg',\n",
       " 'c:\\\\Users\\\\SBA\\\\repository\\\\image_detection\\\\snack_dataOrg_640\\\\1_20241008_102749.jpg',\n",
       " 'c:\\\\Users\\\\SBA\\\\repository\\\\image_detection\\\\snack_dataOrg_640\\\\1_20241008_102755.jpg',\n",
       " 'c:\\\\Users\\\\SBA\\\\repository\\\\image_detection\\\\snack_dataOrg_640\\\\1_20241008_102759.jpg',\n",
       " 'c:\\\\Users\\\\SBA\\\\repository\\\\image_detection\\\\snack_dataOrg_640\\\\2_20241008_102808.jpg',\n",
       " 'c:\\\\Users\\\\SBA\\\\repository\\\\image_detection\\\\snack_dataOrg_640\\\\2_20241008_102812.jpg',\n",
       " 'c:\\\\Users\\\\SBA\\\\repository\\\\image_detection\\\\snack_dataOrg_640\\\\2_20241008_102815.jpg',\n",
       " 'c:\\\\Users\\\\SBA\\\\repository\\\\image_detection\\\\snack_dataOrg_640\\\\2_20241008_102819.jpg',\n",
       " 'c:\\\\Users\\\\SBA\\\\repository\\\\image_detection\\\\snack_dataOrg_640\\\\2_20241008_102821.jpg',\n",
       " 'c:\\\\Users\\\\SBA\\\\repository\\\\image_detection\\\\snack_dataOrg_640\\\\2_20241008_102825.jpg',\n",
       " 'c:\\\\Users\\\\SBA\\\\repository\\\\image_detection\\\\snack_dataOrg_640\\\\3_20241008_102840.jpg',\n",
       " 'c:\\\\Users\\\\SBA\\\\repository\\\\image_detection\\\\snack_dataOrg_640\\\\3_20241008_102843.jpg',\n",
       " 'c:\\\\Users\\\\SBA\\\\repository\\\\image_detection\\\\snack_dataOrg_640\\\\3_20241008_102845.jpg',\n",
       " 'c:\\\\Users\\\\SBA\\\\repository\\\\image_detection\\\\snack_dataOrg_640\\\\3_20241008_102849.jpg',\n",
       " 'c:\\\\Users\\\\SBA\\\\repository\\\\image_detection\\\\snack_dataOrg_640\\\\3_20241008_102852.jpg',\n",
       " 'c:\\\\Users\\\\SBA\\\\repository\\\\image_detection\\\\snack_dataOrg_640\\\\3_20241008_102855.jpg']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63192001-fd44-4939-886c-d57a8fe2758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_label_txt(txtFile):\n",
    "    category_ids = []\n",
    "    bboxes = []\n",
    "\n",
    "    f=open(txtFile,'r')\n",
    "\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line: break\n",
    "        ids, xc, yc, w, h= line.split(' ')\n",
    "        category_ids.append(int(ids))\n",
    "        bboxes.append([float(xc),float(yc),float(w),float(h)])\n",
    "        #print(line)\n",
    "    f.close()\n",
    "    return category_ids, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dd43bb6-ba83-4147-baf1-786d32706f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_label_txt(txtFile, category_ids, bboxes):\n",
    "    f=open(txtFile,'w')\n",
    "\n",
    "    for i, ids in enumerate(category_ids):\n",
    "        xc,yc,w,h = bboxes[i]\n",
    "        f.write(\"{} {} {} {} {}\\n\".format(int(ids),xc,yc,w,h))\n",
    "        #print(\"{} {}\".format(int(ids), bboxes[i]))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7293566e-f74f-40e1-a3de-0d8ed304a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id_to_name = {0: 'snack', 1: 'hershey', 2: 'eclipse'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f3b765-98ba-4196-affd-7d0bb4b76e5e",
   "metadata": {},
   "source": [
    "### 원본 이미지를 축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60473bae-0cd3-418a-8b0f-5b3ed747923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_resize = A.Compose(\n",
    "    [A.LongestMaxSize(max_size=800, interpolation=cv2.INTER_LINEAR),],\n",
    "    bbox_params=A.BboxParams(format='yolo', label_fields=['category_ids']),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6d51dc-182e-4638-8e6e-6d8d6ef83746",
   "metadata": {},
   "source": [
    "### 원본 이미지 좌우 대칭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e94216be-7a23-4b23-ad10-825c40b1da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_hflip = A.Compose(\n",
    "    [A.HorizontalFlip(p=1.0)],\n",
    "    bbox_params=A.BboxParams(format='yolo', label_fields=['category_ids']),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b26a457c-5867-4c74-bd1e-d31fdfa14cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_hsv = A.HueSaturationValue(\n",
    "    hue_shift_limit=(-5,5),\n",
    "    sat_shift_limit=(20,20),\n",
    "    val_shift_limit=(30,30),\n",
    "    p=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3aedd31c-9b83-46c1-a9e8-a9ebe7b74b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_shift = A.Compose(\n",
    "    [A.ShiftScaleRotate(shift_limit=(-0.2, 0.2), scale_limit=(0, 0), rotate_limit=(0, 0),p=1.0)],\n",
    "    bbox_params=A.BboxParams(format='coco', min_area=1024, min_visibility=0.2, label_fields=['category_ids']),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1512ef6a-475b-4c9e-b052-0da5eaa8199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_scale = A.Compose(\n",
    "    [A.ShiftScaleRotate(shift_limit=(0, 0), scale_limit=(-0.2, 0.2), rotate_limit=(0, 0),p=1.0)],\n",
    "    bbox_params=A.BboxParams(format='coco', min_area=1024, min_visibility=0.2, label_fields=['category_ids']),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ff1bf8-5e1e-4723-93cc-7d3d0aa644bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e69e0c1-7279-47c8-bad8-ca7126d7a0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "def dataAug(func, imagefiles):\n",
    "    for imagefile in imagefiles:\n",
    "        image = cv2.imread(imagefile)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        img_height, img_width = image.shape[0:2]\n",
    "    \n",
    "        # txt파일은 이미지 파일에서 확장자만 다르다!\n",
    "        baseName = imagefile.split('.')[0]\n",
    "        txtFile = baseName+'.txt'\n",
    "        category_ids, bboxes = read_label_txt(txtFile)\n",
    "        \n",
    "        if func=='hflip':\n",
    "            transformed = transform_hflip(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "            saveImageName = baseName + '_hflip.JPG'  \n",
    "            saveLabelName = baseName + '_hflip.txt'\n",
    "            saveImage = cv2.cvtColor(transformed['image'], cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite(saveImageName, saveImage)\n",
    "            write_label_txt(saveLabelName, transformed['category_ids'], transformed['bboxes'])            \n",
    "        elif func=='rotate':\n",
    "            angle_inter = 20\n",
    "            for angle in range(angle_inter,360,angle_inter):\n",
    "                transform_rotate = A.Compose(\n",
    "                    [A.Rotate(limit=(angle,angle), rotate_method='ellipse', p=1.0)],\n",
    "                    bbox_params=A.BboxParams(format='yolo', label_fields=['category_ids'])\n",
    "                )\n",
    "                transformed = transform_rotate(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "                saveImageName = baseName + '_rotate_{}.JPG'.format(angle)  \n",
    "                saveLabelName = baseName + '_rotate_{}.txt'.format(angle)  \n",
    "                saveImage = cv2.cvtColor(transformed['image'], cv2.COLOR_RGB2BGR)\n",
    "                cv2.imwrite(saveImageName, saveImage)\n",
    "                write_label_txt(saveLabelName, transformed['category_ids'], transformed['bboxes'])\n",
    "        elif func=='resize':\n",
    "            transformed = transform_resize(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "            saveImageName = baseName + '.JPG'  \n",
    "            saveLabelName = baseName + '.txt'            \n",
    "            saveImage = cv2.cvtColor(transformed['image'], cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite(saveImageName, saveImage)\n",
    "            write_label_txt(saveLabelName, transformed['category_ids'], transformed['bboxes'])\n",
    "        elif func=='hsv':\n",
    "            transformed = transform_hsv(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "            saveImageName = baseName + '_hsv.JPG'  \n",
    "            saveLabelName = baseName + '_hsv.txt'\n",
    "            saveImage = cv2.cvtColor(transformed['image'], cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite(saveImageName, saveImage)\n",
    "            write_label_txt(saveLabelName, transformed['category_ids'], transformed['bboxes'])  \n",
    "        elif func=='shift':\n",
    "            transformed = transform_shift(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "            saveImageName = baseName + '_shift.JPG'  \n",
    "            saveLabelName = baseName + '_shift.txt'\n",
    "            saveImage = cv2.cvtColor(transformed['image'], cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite(saveImageName, saveImage)\n",
    "            write_label_txt(saveLabelName, transformed['category_ids'], transformed['bboxes'])\n",
    "        elif func=='scale':\n",
    "            transformed = transform_scale(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "            saveImageName = baseName + '_scale.JPG'  \n",
    "            saveLabelName = baseName + '_scale.txt'\n",
    "            saveImage = cv2.cvtColor(transformed['image'], cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite(saveImageName, saveImage)\n",
    "            write_label_txt(saveLabelName, transformed['category_ids'], transformed['bboxes'])             \n",
    "            \n",
    "        if DEBUG==True:\n",
    "            visualize(\n",
    "                transformed['image'],\n",
    "                transformed['bboxes'],\n",
    "                transformed['category_ids'],\n",
    "                category_id_to_name,\n",
    "                img_width, img_height\n",
    "            )\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13e05932-e850-4e27-91c3-d5271b08914f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataAug('resize', imagefiles)\n",
    "dataAug('hflip', imagefiles)\n",
    "dataAug('rotate', imagefiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96b3a9d2-aab1-4f45-9f18-667d02e0ea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagefiles2 = glob(os.path.join(orgDataPath, '*.jpg'))\n",
    "dataAug('hsv', imagefiles2)\n",
    "\n",
    "imagefiles3 = glob(os.path.join(orgDataPath, '*.jpg'))\n",
    "dataAug('shift', imagefiles3)\n",
    "\n",
    "imagefiles4 = glob(os.path.join(orgDataPath, '*.jpg'))\n",
    "dataAug('scale', imagefiles4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b679e0b-582d-4ef6-9962-bf94f7b24cbb",
   "metadata": {},
   "source": [
    "### org폴더의 파일들을 train, valid, test로 나누어서 복사한다. 비율 ( 7 : 1.5 : 1.5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdea3ea1-91ec-48f2-8ed6-f72498b93da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2736, 2736)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allImages = glob(os.path.join(orgDataPath, '*.jpg'))\n",
    "allLabels = glob(os.path.join(orgDataPath, '*.txt'))\n",
    "len(allImages), len(allLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdfadef8-ee5b-4efd-bcf8-77ffdf28351c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1915, 410, 411, 2736)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalCount = len(allImages)\n",
    "\n",
    "train_ratio = 0.7\n",
    "valid_ratio = 0.15\n",
    "test_ratio  = 0.15\n",
    "\n",
    "train_size = int(totalCount * train_ratio)\n",
    "valid_size = int(totalCount * valid_ratio)\n",
    "test_size  = int(totalCount * test_ratio)+1\n",
    "train_size, valid_size, test_size, len(allImages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b01a3-eec3-4ecd-8102-174077da35fd",
   "metadata": {},
   "source": [
    "### 데이터셋 나기기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ee3b924-72be-4473-b142-ec929aee5ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "train_dataset, valid_dataset, test_dataset = random_split(allImages, [train_size, valid_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed00178a-1abd-49f4-9b37-2e13249dcdcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\SBA\\\\repository\\\\image_detection\\\\snack_dataOrg_640\\\\2_20241008_102808_rotate_80_hsv_shift_scale.JPG'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f8507e2-6641-4b0b-aee4-3a3bb58d62eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 폴더 생성\n",
    "main_dirs = ['train','valid','test'] \n",
    "sub_dirs  = ['images', 'labels']\n",
    "\n",
    "for main_dir in main_dirs:\n",
    "    for sub_dir in sub_dirs:\n",
    "        dirname = os.path.join(HOME, main_dir, sub_dir)\n",
    "        os.makedirs(dirname, exist_ok=True)\n",
    "\n",
    "        if main_dir=='train' and sub_dir=='images':\n",
    "            for srcPath in train_dataset:\n",
    "                basename = os.path.basename(srcPath)\n",
    "                dstPath = os.path.join(dirname,basename)\n",
    "                shutil.copy(srcPath, dstPath)\n",
    "        elif main_dir=='train' and sub_dir=='labels':\n",
    "            for srcPath in train_dataset:\n",
    "                basename = os.path.basename(srcPath).split('.')[0] + '.txt'\n",
    "                srcPath = os.path.join(orgDataPath,basename)\n",
    "                dstPath = os.path.join(dirname,basename)\n",
    "                shutil.copy(srcPath, dstPath)\n",
    "        elif main_dir=='valid' and sub_dir=='images':\n",
    "            for srcPath in valid_dataset:\n",
    "                basename = os.path.basename(srcPath)\n",
    "                dstPath = os.path.join(dirname,basename)\n",
    "                shutil.copy(srcPath, dstPath)\n",
    "        elif main_dir=='valid' and sub_dir=='labels':\n",
    "            for srcPath in valid_dataset:\n",
    "                basename = os.path.basename(srcPath).split('.')[0] + '.txt'\n",
    "                srcPath = os.path.join(orgDataPath,basename)\n",
    "                dstPath = os.path.join(dirname,basename)\n",
    "                shutil.copy(srcPath, dstPath)\n",
    "        elif main_dir=='test' and sub_dir=='images':\n",
    "            for srcPath in test_dataset:\n",
    "                basename = os.path.basename(srcPath)\n",
    "                dstPath = os.path.join(dirname,basename)\n",
    "                shutil.copy(srcPath, dstPath)\n",
    "        elif main_dir=='test' and sub_dir=='labels':\n",
    "            for srcPath in test_dataset:\n",
    "                basename = os.path.basename(srcPath).split('.')[0] + '.txt'\n",
    "                srcPath = os.path.join(orgDataPath,basename)\n",
    "                dstPath = os.path.join(dirname,basename)\n",
    "                shutil.copy(srcPath, dstPath)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "42504917-0446-455d-aa2c-f384e356cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!yolo train model=yolo11n.pt data=C:/Users/park0/yolov11/customData/data.yaml epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e793aebd-cd11-4cf4-b752-3683800b1cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.9 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.6  Python-3.10.15 torch-2.4.1+cpu CPU (12th Gen Intel Core(TM) i7-12700)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=C:/Users/SBA/repository/image_detection/customdata/data.yaml, epochs=100, time=None, patience=100, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=1st_albu_8_n_1660, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\1st_albu_8_n_1660\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    431257  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,425 parameters, 2,590,409 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\SBA\\repository\\image_detection\\customdata\\train\\labels.cache... 1915 images, 1442 backgrounds, 0 corrupt: 100%|██████████| 1915/1915 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\SBA\\repository\\image_detection\\customdata\\valid\\labels.cache... 410 images, 301 backgrounds, 0 corrupt: 100%|██████████| 410/410 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\1st_albu_8_n_1660\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\1st_albu_8_n_1660\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/100         0G      1.508      7.172       1.72          1        640: 100%|██████████| 240/240 [04:56<00:00,  1.23s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 26/26 [00:29<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        410        109      0.136        0.3      0.132     0.0486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      2/100         0G      1.698       6.55      1.854          1        640: 100%|██████████| 240/240 [04:51<00:00,  1.21s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 26/26 [00:23<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        410        109     0.0879      0.108     0.0689     0.0291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      3/100         0G      1.779      4.002      1.931          2        640: 100%|██████████| 240/240 [04:48<00:00,  1.20s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 26/26 [00:24<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        410        109      0.399      0.327      0.156      0.057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      4/100         0G       1.82      3.476      1.865          2        640: 100%|██████████| 240/240 [04:44<00:00,  1.19s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 26/26 [00:23<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        410        109      0.184      0.581      0.178     0.0679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      5/100         0G      1.758      3.061      1.854          1        640: 100%|██████████| 240/240 [04:44<00:00,  1.18s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 26/26 [00:23<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        410        109      0.398      0.337      0.232     0.0968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      6/100         0G      1.711       2.68      1.794          2        640: 100%|██████████| 240/240 [04:44<00:00,  1.18s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 26/26 [00:24<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        410        109      0.174      0.741      0.233     0.0987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      7/100         0G       1.69      2.622      1.762          1        640: 100%|██████████| 240/240 [04:44<00:00,  1.18s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 26/26 [00:23<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        410        109      0.239       0.74      0.286      0.152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      8/100         0G      1.682      2.562      1.773          4        640: 100%|██████████| 240/240 [04:43<00:00,  1.18s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 26/26 [00:23<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        410        109      0.121      0.467      0.141     0.0551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      9/100         0G      1.635      2.455      1.721          5        640:  85%|████████▌ | 205/240 [04:06<00:42,  1.20s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolo11n.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# load a pretrained model (recommended for training)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/SBA/repository/image_detection/customdata/data.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1st_albu_8_n_1660\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SBA\\anaconda3\\envs\\yolov11\\lib\\site-packages\\ultralytics\\engine\\model.py:802\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 802\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\SBA\\anaconda3\\envs\\yolov11\\lib\\site-packages\\ultralytics\\engine\\trainer.py:207\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SBA\\anaconda3\\envs\\yolov11\\lib\\site-packages\\ultralytics\\engine\\trainer.py:393\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    389\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m*\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items) \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items\n\u001b[0;32m    390\u001b[0m     )\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ni \u001b[38;5;241m-\u001b[39m last_opt_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate:\n",
      "File \u001b[1;32mc:\\Users\\SBA\\anaconda3\\envs\\yolov11\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SBA\\anaconda3\\envs\\yolov11\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SBA\\anaconda3\\envs\\yolov11\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data=\"C:/Users/SBA/repository/image_detection/customdata/data.yaml\", epochs=100, batch = 8, name = '1st_albu_8_n_1660')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72590e3a-ae6e-45af-82dd-e78702dbfd1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
